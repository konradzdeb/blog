---
title: Bring your Python ML model to your iOS App in Three Minutes
author: Konrad Zdeb
date: '2025-05-02'
slug: python-models-ape
categories:
  - how-to
tags:
  - swift
  - ML
  - Python
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
```



Arrival of CoreML Framework in June 2017 open up an exciting possibility of productionings models on Apple devices. This is particularly attractive in the context of deploying ML-reliant applications on mobile devices, offering access to a significant market. In order to introduce a ML solution into a Swift-based software product `.mlmodel` file would need to be produced. The two common mechanisms facilitating that process are:
1. Leveraging Apple's Create ML App or Create ML framework (for programmatic model creation)
2. Leveraging `coremltools` Python package and preparing model elsewhere to be imported into the production


## Model Development

For the purpose of demonstration we will create basic modelling solution in Python. As I'm looking to classify some images, I will create a simple Convolutional Neural Network (CNN) using PyTorch. The model will be trained on Fashion-MNIST dataset, which is a collection of 70,000 grayscale images of fashion items (clothing, shoes, etc.) in 10 categories. I will start from sourcing a standard set of packages required for the model development.

```{r show_python_model_trainig, echo=FALSE, results='asis'}
base_dir <- "../../../../../Swift-Xcode/SwiftPythonML/PythonMLModel"
code <- readLines(file.path(base_dir, "model_prep.py"))
cat("```python\n", paste(trimws(code[1:16]), collapse = "\n"), "\n```")
```

The model represents a fairly unsophisticated approach to handle imaghe classification task. Naturally, in a producting setting you will want to utilise more sophisticated solution, handling complex data and scenarios where you could be dealing with distorted images data (low lighting, different angles, etc.). The provided CNN implementation is fairly basic but sufficient for the purpose of this demonstration. It consists of a few convolutional layers, followed by fully connected layers, and uses ReLU activation functions. The model is trained using the Adam optimizer and cross-entropy loss function.

```{r show_python_model_dev, echo=FALSE, results='asis'}
cat("```python\n", paste(code[17:82], collapse = "\n"), "\n```")
```
### Testing the Model
In addittion to testing the model for performance, we also tet model's ability to handle images passed as flat files. For that purpose, I will build trivial ficture in PyTest to feed a few images to the model and check if the model is able to predict the label of the image. The test will be run against a few images sourced from public domain.

```{r show_python_model_testing, echo=FALSE, results='asis'}
# Helper function to construct the path to the test file
code_test <- readLines(file.path(base_dir, "tests/test_model.py"))
cat("```python\n", paste(code_test, collapse = "\n"), "\n```")
```

### Convert to Core ML

The key challenge would be brining the model into the  Swift and incorporating the modelling solution in the iOS application. We will export model to the `.mlpackage` format using avilable converters. The key element we want to take care of ensuring that our model will be able to handle the required input format. In this case images, we accomplish that objective by defining the `inpute_features` and `output_features`.

The point that deserves most attention relates to how the `input_features` list is being created. This object is critical when converting a scikit-learn model to Core ML format using `coremltools`. In this example, the input features are defined as `input_features = [("image", ct.models.datatypes.Array(1, 28, 28))]`, which means the Core ML model expects a single-channel (grayscale) image of size 28x28 as input. This matches the shape of Fashion-MNIST images and ensures the model can process image data correctly in your iOS application. 

Why this is important? When you convert a scikit-learn model to Core ML, the input features must match the expected input shape of the model. If the input features are not defined correctly, the conversion will fail or the resulting Core ML model will not work as intended in your iOS application.

```{r show_python_model_convert, echo=FALSE, results='asis'}
cat("```python\n", paste(code[83:length(code)], collapse = "\n"), "\n```")
```

## Use in Swift

Initially we will need to import the model to XCode project. This can be done by dragging and dropping the `.mlpackage` file into the Xcode project navigator. Once the model is imported, we can use it in our Swift code. Upon the import the model will be available as a class with the same name as the `.mlpackage` file. In this case, it will be `FashionMNISTClassifier`.
```{r show_swift_model_import, echo=FALSE, results='asis'}
xcode_base_dir <- "../../../../../Swift-Xcode/SwiftPythonML/PythonMLModel"
```