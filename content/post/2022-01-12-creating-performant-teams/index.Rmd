---
title: Creating Highly Performant Data Science Teams
author: Konrad Zdeb
date: '2022-01-12'
slug: creating-performant-teams
categories:
  - business
tags:
  - management
  - team
---

Creating highly performance data science teams is a subject that presently occupies a lot of business thinking. A subsntial amount of literature concerned with creating efficient software development techniques exists but the subject of efficient data science teams is receiving somhow smaller attention. A partial reason behind this may be relatively new nature of the teams 

## What's different data science

What data science team for an organisation can be understood on three layers:

* On a **conceptual** layer a data science team answers certain types of question, such how many customers may leave organisation if change pricing on product
* On an **analytical layer** data science team "models" those answers, in practice that boils down to old good statistical analysis where
* On an **artefact** level the team will be responsible for generating
  * Modelling code, such as Python, Java, R and other implementations
  * Statistical reports outlining methodologies
  * Documentation for generated data
  * Result summaries
    * In form of interactive dashboards
    * Periodical reports

Personally, I would argue against a notion where role of Data Science Team is defined as productionisg Machine Learning solutions. Such ramification narrow down the scope of the team and make the structure inherently more efficient. A substantial data science may have a designed function responsible for ML implementation. As such that function would focus on certain type of modelling in opposition to other unit that may hone expertise in space-time modelling and so forth. Any data science team should be open to embrace statistical techniques that solve the problem best, not fit the problem to the modelling pipeline. If this is not stated, we are running a risk of creating a team that will attempt to solve even most trival challenges using sophisticated approach where a simpler approach would suffice.

## Data Science Team in business

In this post, I want to focus on the **artefact** layer of data science activity but before doing so I would like to outline how the engagement of the data science team with business actually happens. 

### Example

Let's consider a hypothetical example of a company that sells widget subscription and wants to build attrition modelling solution. In terms of a flow, we can imagine this process looking similar to the one below. 

```{r trad_flow_diagram, echo=FALSE, strip.white=TRUE}
DiagrammeR::mermaid(diagram = "trad_flow.mmd")
```

In terms of practical examples, usually we would expect for things on the lines of the below points to take place during each of the stages. In practice, some of the events may be compressed for instance business may be aware of feasibility behind the original solution. There is an ample amount of literature on ML development cycles so I'm not giving details of that process^[A robust overview of data science development pipeline is ].

```{r flow_table, echo=FALSE}
library(magrittr)
library(kableExtra)
library(tibble)
data_flow <- tribble(
    ~Stage, ~Example,
    "Business Requirements", "Business wants to identify customers likely to leave",
    "Analytical Conceptualisation", "Classical supervised learning approach. We have some data on customers that already left and data on how customers use our service.",
    "Business Acceptance", "Viable, we are on track with initial requirement. Clarification was needed that instead of providing TRUE / FALSE flag on customers being likely to leave it may be more informative to provide a percentage. Business then can decide to approach customers that are 90% or more likely to leave first.",
    "Exploratory Analysis", "Usually at this stage some informal analysis took place. In this stage data science team would look to provide some initial insight on how robust is the solution.",
    "Data Access", "Two things will happen at this stage. The team will seek data to train the initial modelling solution and assure that this form of access is usuable in production.",
    "Modelling", "The usual modelling cycle. "
)
kable(data_flow, booktabs = TRUE) %>% pack_rows(
  index = c("Idea Formulation" = 4, "Development" = 2))

```

## Efficiency

I would imagine that people may have a variable view on the process outlined above but we would agree that majority of elements are to a greater or smaller degree present across modern business environment. For the sake of brevity, I haven't dive deep into actual process around development such as code generation, peer review, test building and so on but I will do a little bit of this now. In order to understand whether a given team is efficient and if it could be mode more we have to gain a deep insight into the artefacts that team generates and processes around it. Functional / engagement perspective only gives us part of the definition. To say that data science team delivers a "modelling solution" is comparable to saying that local sandwich maker and Gordon Ramsay are "making food". 

This is true in a classical logic sense but doesn't give a full picture as they are different levels of sophistication and complexity. Let's look at the team workflow from **an artefact** generation perspective.



