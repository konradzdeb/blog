---
title: Using R for File Manipulation
author: Konrad Zdeb
date: '2021-03-29'
slug: file-manipulation-in-r
draft: true
categories:
  - how-to
tags:
  - R
references:
- id: Christensson
  title: One-click science marketing
  author:
  - family: Christensson
    given: P.
  URL: 'https://techterms.com'
  publisher: Nature Publishing Group
  type: webpage
  issued:
      year: 2011
      month: September
      day: 23
---

# Challenge

File manipulation is a frequent task unavoidable in almost every IT business process. Traditionally, file manipulation tasks are accomplished within the ramifications of specific tools native to a given system. As such, the one may consider writing and scheduling shell script to undertake frequent file operations or using more specific purpose-built tools like [`logrotate`](https://linux.die.net/man/8/logrotate) in order to archive logs or tools like [Kafka](https://kafka.apache.org/20/documentation.html) are used to build streaming-data pipelines. R is usually though of as a statistical programming language or as an environment for a statistical analysis. The fact that R is a mature programming language able to successfully accomplish a wide array of traditional tasks is frequently ignored. What constitutes a *programming language* is a valid question. Wikipedia offers somehow wide definition:

> A programming language is a formal language comprising a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms.

[*Wikipedia*](https://en.wikipedia.org/wiki/Programming_language)

# Pros and Cons of Using R as an ETL solution

* The article assumes that consideration on whether to utilise R within a team for pure ETL tasks is usually given in the context of R language being utilised to greater or lesser degree to facilitate data modelling, analytical or statistical work. R wasn't designed to replace ETL^[Export Transform Load] processes; nevertheless, as every other well-developed programming language R offers a set of robust tools for accomplishing file manipulation, analysis and creation. In particular, R package ecosystem offers a layer that provides a clean, **unified,** interface for accomplish a variety of tasks that, traditionally, would be solutioned using system tools or bespoke applications.

## Pros

* R is an exceptionally flexible language and offered data structures lend themselves well for file manipulation
* Ability for our production process to be migrated across the systems and configurations increases. Let's consider a simple example of generating temporary files. Utilising R base function `tempfile` or equivalent call from the `fs` package, `fs::file_temp()` allows for swift creation of temporary files. Achieving that on MS Windows using PowerShell could be done using specific cmdlet  called New-TemporaryFile `$tmp = New-TemporaryFile`. When working in command prompt we would leverage `%TEMP%` and `%RANDOM%` environment variables to come up with something like `set "uniqueFileName=%tmp%\bat~%RANDOM%.tmp"`, as discussed at lengths in [the related Stack Overflow answer](https://stackoverflow.com/a/32109191/1655567). The point is that exercises like that incur addittional maintaince cost. Whereas `tempfile` is straightfoward, well documented and understood by every R user, call:
```{coffee eval=FALSE, include=TRUE}
@echo off
setlocal EnableExtensions

rem get unique file name 
:uniqLoop
set "uniqueFileName=%tmp%\bat~%RANDOM%.tmp"
if exist "%uniqueFileName%" goto :uniqLoop
```
won't be obvious to maintainers not familiar with command prompt, even if the actual goal behind the two is identical from a production perspective (to provide process with a temporary path)

* Thanks to the [Rocker Project](https://www.rocker-project.org) R plays exceptionally well with Docker which is conducive to reducing future maintainability costs and enhancing portability. Decent article covering technicalities of using Docker with R was provided by Colin Fay.^[Colin Fay; [*An Introduction to Docker for R Users*](https://colinfay.me/docker-r-reproducibility/).]

* R has rich ecosystem offering API connectivity. Thanks to [*Plumber*](https://www.rplumber.io) R users can, with relatively little effort, generate own APIs. Packages like [*wbstats*](https://github.com/gshs-ornl/wbstats) and [*Eurostat*](https://github.com/rOpenGov/eurostat) provide convenient wrappers for sourcing data from publicly available repositories (article World Bank and Eurostat). Those can be immensely time server if we consider building processes that rely on background external demographic/macroeconomic data being incorporated in our processes on regular basis. The subject of R / API integration merits a separate article.

* File format converstion. Through packages like [`haven`](https://haven.tidyverse.org/) R offers rich interface to access data in different formats (Stata, SPSS, etc.). When working with bloated legacy processes that may required handling input originating from heterogeneous sources (think SAS binary, in-house Oracle database and some periodical figures produced by World Bank) the R flexibility may prove actual life saver. Still we should be careful not to use R as an excuse not to refactor old processes. 

## Cons

* Using R for file manipulation is not suitable for processes that are mostly in vast majority ETL tasks. If our intention is to monitor a location for change and then conditionally copy files building R-based process will incur an unnecessary dependency, whereas more tools would 

* Achieving fine-grained control over common operations may be more difficult in R. Let's consider `rsync`. The Internet is full of examples how to achieve properly defined goals using `rsync`, it is also possible to find Python implementation of the algorithm^[An interesting implementation was offered by Tyler Cipriani in [his blog](https://tylercipriani.com/blog/2017/07/09/the-rsync-algorithm-in-python/).]. At the time of writing this article, the attempts to bring `rsync` functionality relied on command line tool.^[R package providing a convenient wrapper around `rsync` is available through the GitHub repo: [INWTlab/rsync](https://github.com/INWTlab/rsync).]

* Multithreading, the thinking around multithreading and asynchronous computation in R oscilliates around finding more efficient ways to accomplish computational effort. Whereas packages like [r-lib/async](https://github.com/gaborcsardi/async) or [r-lib/processx](https://github.com/r-lib/processx) offer aproaches to run processes in the backrgound or perform asynchronous I/O operations; those solutions are applicable to more complex process. Achieveing parallel `rsync` execution can be easily achieved using `xargs`, as found on Stack Overflow^[The answer the question on *Speed up rsync with Simultaneous/Concurrent File Transfers?* available [here](https://stackoverflow.com/a/25532027/1655567).]:
```{bash eval=FALSE, include=TRUE}
ls /srv/mail | xargs -n1 -P4 -I% rsync -Pa % myserver.com:/srv/mail/
```

## Packages worth looking at

When discussing efficiencies derivable from R in the context of file manipulation it's woth to mention a suitable packages as available 

```{r pkgs_table, echo=FALSE}
knitr::kable(
  tibble::tribble(
    ~Package, ~Description,
    "[fs](https://github.com/r-lib/fs)", "Cross-platform, uniform interface for file manipulation",
    "[processx](https://github.com/r-lib/processx)", "Executing and controlling system processes",
    "[zip](https://github.com/r-lib/zip)", "Cross-platform zip compression in R",
    "[rsync](https://github.com/INWTlab/rsync)", "R wrapper around rsync",
    "[sftp](https://github.com/stenevang/sftp/)", "SFTP for R",
    "[eurostat](https://ropengov.github.io/eurostat/), [wbstats](https://github.com/gshs-ornl/wbstats)", "For accessing publicly available data"
    )
)
```


# Conclusion

The R-based file manipulation process are only efficient, from a production perspective, when applied to processes containing a substantial analytical/statistical component. 

# References

https://techterms.com/definition/programming_language
