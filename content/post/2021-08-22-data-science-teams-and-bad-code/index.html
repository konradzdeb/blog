---
title: Why Data Science Teams Write Notoriously Bad Code
author: Konrad Zdeb
date: '2021-08-22'
slug: data-science-teams-and-bad-code
draft: true
categories:
  - business
tags:
  - practice
  - coding
  - workflow
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="the-problem" class="section level1">
<h1>The problem …</h1>
<p>What is most worrisome is not the fact that data science teams frequently deliver an average quality code but the fact that this is very problem is often ignored by business. The challenge has complex roots and can be traced to a few wider key characteristics associated with the place of data science team in business. Broadly, those will be related to the following aspects:</p>
<ul>
<li>Functional location of the data science team that is too remote from the proper IT process</li>
<li>Lack of independent objectives focusing on code quality</li>
<li>Overall training of data scientist</li>
</ul>
</div>
<div id="good-code-bad-code" class="section level1">
<h1>Good Code / Bad Code</h1>
<p>Before slagging off data science teams from writing bad code it’s required to consider what are the hallmarks of quality code and what workflow characteristics are conducive to obtaining high quality code. There is no one definition of “high quality code” that would be uniformly accepted, as discussed in a related online conversation<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> one of the best available measures is “WTFs per minute”</p>
<div class="figure">
<img src="images/wtfm.jpg" width="500" height="471" alt="" />
<p class="caption"><em>WTFs per minute</em></p>
</div>
<p>The relevant academic literature and highlights aspects like test coverage, cyclomatic complexity<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>, duplication / violations of do not repeat yourself principle (DRY) and other elements like quality of comments.</p>
<p>In a software-oriented business (i.e. think of a software house that makes living out of churning mobile phone apps) we would pick a number of characteristics that constitute good coding environment. The excellent article defining wider business characteristics was published by <a href="https://www.joelonsoftware.com">Joel Spolsky</a> and focuses on characteristics we would like to see in a well-organised software development business. Leaving on side wider office dynamic, from a coding perspective in a good workflow we would anticipate to find the following elements properly defined</p>
<ul>
<li>Does the team have a clarity on what is in and out of their tech stack? Imagine that you have a project where ETL is delivered via SAS, ML is programmed in Python and report is generated through R using RMarkdown. The immediate question that arises is why? Are all of those technologies necessary? No, very seldom there is a justifiable technical reason for that state of affairs to emerge; however, surprisingly this is not infrequent. I will explain later why this happens</li>
<li>Is there a peer review process that would be enacted in a transparent, permanent manner?</li>
<li>Is there a searchable dynamic documentation?</li>
<li>Do all developers understand unit testing?</li>
<li>How about continuous integration?</li>
<li>Is the written code modular, re-usable?</li>
</ul>
<p>In a well functioning software development house we would find that those items are usually well managed. The question that arises is why this is not common across data science teams?</p>
</div>
<div id="what-are-data-scientists-frequently-asked-to-do" class="section level1">
<h1>What are data scientists (frequently) asked to do</h1>
<p>The question on why those practices are not frequently followed can be partially answered by why are the key objectives for decision science teams.</p>
<div id="discovery-vs.-discovering" class="section level2">
<h2>Discovery vs. Discovering</h2>
<p>The objective that data science are frequently tasked with is to discover something. Let’s imagine that we run a mobile phone company and we would like build a customer attrition model. The task presented to data science team would be too look at the wealth of data on customer characteristics, activity and come up with a modelling solution flagging customers that are likely to leave the business. It’s to imagine that this can be done through a supervised ML approach. We have details on customers that left the business, we could pick similar patterns across our customer base and, with some degree of confidence, flag those that are likely to leave. So far so good, the next step would be to surface that flag via our CRM system and let our marketing / customer folk approach those customers with an offer inducing them to stay. Naturally, there is whole income/transactional cost element here but that’s not the focus. From a quality of coding perspective major mistake was made.</p>
<p>The data science team was asked to deliver <em>a solution</em> not <em>a process for obtaining the solution.</em> What that means in practice that if we want to try a different model, expand base data or look at different time frames / subsets we have to liaise with our data science team to accomplish that task. More importantly, because we assess our data science team on quality of the modelling outputs (i.e. confusion matrix) they were given (too) much freedom on how to do things. In all likelihood, we don’t have a reusable piece of software but a set of analytical scripts, or worse, Jupyter notebooks that do everything from ETL to visualisations and exporting results. Those things are nightmare to support. We are in this mess because we asked the team to <em>discover</em> a solution not give use a tool that will enable us to keep <em>discovering</em> the business relevant insights again and again.</p>
<p>If we were to ask our data science team to deliver a <em>tool</em> that enables to run different attrition models on heterogeneous data tools the task would be much more complex in a short term but significantly more beneficial to the business. Delivery of a <em>tool</em> forces a whole set of requirements that are not surfaced in an analytical project. Tool has to have user-facing documentation, interface, come with some dependency management system (think <code>requirements.txt</code> for Python-based or <code>DESCRIPTION</code> files for R package). Tool has to be sufficiently data agnostic. We can impose certain requirements concerning the data format but the whole purpose of a tool is that it will deployable again and again against different data subsets.</p>
</div>
<div id="what-happens-at-uni-stays-at-uni" class="section level2">
<h2>What happens at Uni stays at Uni …</h2>
<p>Unfortunately, the phrase <strong>What happens in Vegas stays in Vegas</strong> does not apply well to degree/business pipeline for data science. Data science degrees are organised in fairly similar manner, students get exposure to a number of fairly complex statistical concepts, there is some greater or smaller of focusing on coding in one of the popular languages (R, Python, maybe some Java or Scala) to finish off with a big project that does some ML on some ‘topical’ data, think Kaggle Competitions. What is problematic in this approach that the whole body of knowledge on software development cycle was either ignored or given very little attention. The difference between business and academic is in the ’<em>ing’ element of work. The business has a need for data scientists to keep </em>discovering* not to <em>discover.</em> If it may be acceptable for a MSc student to manually download a bunch of CSVs read them in via <code>read_csv</code> in R and code ML that very step should be considered as unacceptable in business setting. In business, where almost always, our requirement is to deliver knowledge continuously this approach will get us following problems:</p>
<ul>
<li><p>We have created a process that in perpetuity will rely on someone downloading those bloody files from somewhere in order to refresh the model. Could we use API, read directly from S3 using Spark, automatically copy from SFTP to temp or communicate with data store through API. Almost always ‘yes’ but that means time, coding and effort. All of those activities are seen as ‘waste’ as what business (mistakenly) expects from data science team is a solution not <em>a mechanism for obtaining solutions.</em> Almost always we will end up wasting more time refreshing this insight than it would take time to code this properly</p></li>
<li><p>Maintenance is problematic, in all likelihood we have a bunch of scripts crammed together. The code is not modular and in order to modify the model we can just call in different modelling function. There is no continuous integration / unit testing - we find out if the stuff works after we re-run it, another waste of time</p></li>
</ul>
</div>
</div>
<div id="solutions" class="section level1">
<h1>Solutions</h1>
<div id="no-pain-no-gain" class="section level2">
<h2>No pain no gain …</h2>
<p>Humility is often helpful. Eminent developers are often keen to learn new things and are modest in their self-assessment. You can often see really excellent people like XXXX speaking of learning this or that. That’s a brilliant attitude. Good manager will be able to pull the team towards the learning journey. Degree in a subject is a first step not the last one on journey to becoming a good programmer. This is a challenge, especially for people coming from stats/maths background that usually seen statistical software to ‘run my regression’ and where never challenged to reflect on memory management aspect of the <code>lm</code> function they may be using. Surely, if you run your stuff once at home to finish your dissertation it doesn’t matter if you it takes a minute or five and if you have to spend another few minutes to change the hard-coded paths.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><em>How to Quantify Code Quality.</em> <a href="https://softwareengineering.stackexchange.com/q/400913">softwareengineering.stackexchange.com/q/400913</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>In simplest terms <em>cyclomatic complexity</em> reflects the number of linearly independent paths through a program’s source code.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
